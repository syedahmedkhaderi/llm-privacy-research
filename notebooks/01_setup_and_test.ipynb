{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPNUC+u7CwtuYCCP/SOAGno",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/syedahmedkhaderi/llm-privacy-research/blob/main/notebooks/01_setup_and_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cloning the Repository I work on:"
      ],
      "metadata": {
        "id": "7dRoKYozHsvl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "rQRmTSLJHOw9",
        "outputId": "5950f0b1-11aa-473e-830e-183799b855a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'llm-privacy-research'...\n",
            "remote: Enumerating objects: 58, done.\u001b[K\n",
            "remote: Counting objects: 100% (58/58), done.\u001b[K\n",
            "remote: Compressing objects: 100% (42/42), done.\u001b[K\n",
            "remote: Total 58 (delta 12), reused 52 (delta 10), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (58/58), 10.86 KiB | 5.43 MiB/s, done.\n",
            "Resolving deltas: 100% (12/12), done.\n",
            "/content/llm-privacy-research\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/syedahmedkhaderi/llm-privacy-research.git\n",
        "%cd llm-privacy-research"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cloning the Repository Sent by Professor:"
      ],
      "metadata": {
        "id": "UB3jLPJIHjVs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/HKUST-KnowComp/PrivLM-Bench.git\n",
        "%cd PrivLM-Bench"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "GOYn6eoYHfy2",
        "outputId": "99ebc5d9-ce98-4e34-dbd4-0bb714573592"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'PrivLM-Bench'...\n",
            "remote: Enumerating objects: 154, done.\u001b[K\n",
            "remote: Counting objects: 100% (154/154), done.\u001b[K\n",
            "remote: Compressing objects: 100% (135/135), done.\u001b[K\n",
            "remote: Total 154 (delta 33), reused 128 (delta 18), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (154/154), 226.89 KiB | 1.76 MiB/s, done.\n",
            "Resolving deltas: 100% (33/33), done.\n",
            "/content/llm-privacy-research/PrivLM-Bench\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installing the needed packages reccomended by AI"
      ],
      "metadata": {
        "id": "b4OtlHAUJL9D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q torch transformers datasets evaluate\n",
        "!pip install -q scikit-learn matplotlib seaborn pandas numpy\n",
        "!pip install -q accelerate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YIbqTe1PIYuc",
        "outputId": "e7f26564-0e12-4fbb-967c-29bc8001dc81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/84.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now lets verify the installation of packages"
      ],
      "metadata": {
        "id": "6n2coPPuJRu5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import transformers\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"Transformers version: {transformers.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "\n",
        "# Observation: If GPU not set then CUDA shows 'False' in the output. Therefore go over to runtime and set it to T4."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tdjpt9cYJWkE",
        "outputId": "db339a18-d3b9-4ad7-e17a-0345b56c5e00"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch version: 2.8.0+cu126\n",
            "Transformers version: 4.57.1\n",
            "CUDA available: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets Download a pre-trained language model - GPT-2"
      ],
      "metadata": {
        "id": "lJ39Y-e7JeRp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "print(\"Loading GPT-2 small model...\")\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "print(\"✓ Model loaded successfully!\")"
      ],
      "metadata": {
        "id": "bstuOUNNJo_Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c80d01c8-4851-4470-9498-92e1d208fb3b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading GPT-2 small model...\n",
            "✓ Model loaded successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing the model and Its basic generation"
      ],
      "metadata": {
        "id": "B9M1vAAWLAwe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"My name is\"\n",
        "inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "outputs = model.generate(**inputs, max_length=20)\n",
        "generated_text = tokenizer.decode(outputs[0])\n",
        "print(f\"Input: {text}\")\n",
        "print(f\"Generated: {generated_text}\")"
      ],
      "metadata": {
        "id": "bZQIdCe3LEYF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}